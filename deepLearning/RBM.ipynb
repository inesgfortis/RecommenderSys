{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86e6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../movies')\n",
    "from movieLens import MovieLens\n",
    "\n",
    "# Load the movie Lens class\n",
    "ml = MovieLens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ed2cf",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ff29be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, accuracy, AlgoBase, PredictionImpossible, NormalPredictor\n",
    "from surprise.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54144b6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ratings dataset\n",
    "ratings = ml.ratings.copy()\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028a0fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.0, 5.0, 3.0, 2.0, 1.0, 4.5, 3.5, 2.5, 0.5, 1.5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ratings['rating'].nunique())\n",
    "list(ratings['rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20bc271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method from the Surprise library to load the DataFrame \n",
    "# Define the Reader object to parse the dataframe\n",
    "reader = Reader(rating_scale=(ratings['rating'].min(), ratings['rating'].max()))\n",
    "\n",
    "# Load the dataframe as a ratings dataset\n",
    "ratingsDataset = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Build the full trainset\n",
    "trainSet, testSet = train_test_split(ratingsDataset, test_size=0.2, random_state=42)\n",
    "antitest = trainSet.build_anti_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3702c47",
   "metadata": {},
   "source": [
    "# Prueba 1: OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952d3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RBMAlgorithm import RBMAlgorithm\n",
    "RBM = RBMAlgorithm(epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ba6fe98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained epoch  0\n",
      "Trained epoch  1\n",
      "Trained epoch  2\n",
      "Trained epoch  3\n",
      "Trained epoch  4\n",
      "Trained epoch  5\n",
      "Trained epoch  6\n",
      "Trained epoch  7\n",
      "Trained epoch  8\n",
      "Trained epoch  9\n",
      "Trained epoch  10\n",
      "Trained epoch  11\n",
      "Trained epoch  12\n",
      "Trained epoch  13\n",
      "Trained epoch  14\n",
      "Trained epoch  15\n",
      "Trained epoch  16\n",
      "Trained epoch  17\n",
      "Trained epoch  18\n",
      "Trained epoch  19\n",
      "Processing user  0\n",
      "Processing user  50\n",
      "Processing user  100\n",
      "Processing user  150\n",
      "Processing user  200\n",
      "Processing user  250\n",
      "Processing user  300\n",
      "Processing user  350\n",
      "Processing user  400\n",
      "Processing user  450\n",
      "Processing user  500\n",
      "Processing user  550\n",
      "Processing user  600\n"
     ]
    }
   ],
   "source": [
    "# Fit the RBM model on the training set\n",
    "RBM.fit(trainSet)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = RBM.test(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "084ca154",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predtest = RBM.test(testSet)\n",
    "predantitest = RBM.test(antitest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5468c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefb3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb48d92",
   "metadata": {},
   "source": [
    "# Prueba 2: OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded3179",
   "metadata": {},
   "source": [
    "Para preparar los datos de train y test, necesitamos crear conjuntos en formato de matriz con cada fila representando a un usuario y cada celda de la fila representando la valoración de cada película, dado que esta es la entrada esperada por el algoritmo RBM.\n",
    "\n",
    "Para ello, necesitamos el número total de usuarios como números de fila y el número total de películas como número de columna\n",
    "* num_users = trainSet.n_users\n",
    "* num_movies = trainSet.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95cca135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitData(trainset):\n",
    "    #AlgoBase.fit(self, trainset)\n",
    "\n",
    "    num_users = trainset.n_users\n",
    "    num_movies = trainset.n_items\n",
    "\n",
    "    # 3D matrix: users, movies and ratings\n",
    "    # Ratings has size 10 given the possible rating values\n",
    "    trainingMatrix = np.zeros([num_users, num_movies, 10], dtype=np.float32)\n",
    "\n",
    "    for (uid, iid, rating) in trainset.all_ratings():\n",
    "        adjustedRating = int(float(rating)*2.0) - 1\n",
    "        trainingMatrix[int(uid), int(iid), adjustedRating] = 1\n",
    "\n",
    "    # Flatten to a 2D array, with nodes for each possible rating type on each possible item, for every user.\n",
    "    trainingMatrix = np.reshape(trainingMatrix, [trainingMatrix.shape[0], -1])\n",
    "\n",
    "    return trainingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e56e7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 89280)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingMatrix = fitData(trainSet)\n",
    "trainingMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07c0657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Updated on Sun Dec 1 08:32:13 2019\n",
    "\n",
    "@author: Frank\n",
    "@modified: Saurabh\n",
    "\n",
    "@modified: Inés Gómez Fortis\n",
    "\"\"\"\n",
    "\n",
    "class RBM_v2(AlgoBase):\n",
    "\n",
    "    def __init__(self, visibleDimensions, epochs=20, hiddenDimensions=50, ratingValues=10, learningRate=0.001, batchSize=100):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.visibleDimensions = visibleDimensions\n",
    "        self.epochs = epochs\n",
    "        self.hiddenDimensions = hiddenDimensions\n",
    "        self.ratingValues = ratingValues\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "    def fit(self, trainset, trainingMatrix):\n",
    "        \n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.Train(trainingMatrix)\n",
    "        \n",
    "        num_users = trainset.n_users\n",
    "        num_movies = trainset.n_items\n",
    "               \n",
    "        self.predictedRatings = np.zeros([num_users, num_movies], dtype=np.float32)\n",
    "        for uiid in range(trainset.n_users):\n",
    "            if (uiid % 50 == 0):\n",
    "                print(\"Processing user \", uiid)\n",
    "            recs = self.GetRecommendations([trainingMatrix[uiid]])\n",
    "            recs = np.reshape(recs, [num_movies, 10])\n",
    "            \n",
    "            for itemID, rec in enumerate(recs):\n",
    "                # The obvious thing would be to just take the rating with the highest score: rating = rec.argmax()\n",
    "                # but this just leads to a huge multi-way tie for 5-star predictions.\n",
    "                # Instead, the paper suggests performing normalization over K values to get probabilities and take the expectation as the prediction\n",
    "                normalized = np.exp(rec)/np.sum(np.exp(rec), axis=0)\n",
    "                rating = np.average(np.arange(10), weights=normalized)\n",
    "                self.predictedRatings[uiid, itemID] = (rating + 1) * 0.5\n",
    "        \n",
    "        return self        \n",
    "        \n",
    "\n",
    "    # in order to reconstract any users rating for any item            \n",
    "    def Train(self, X):\n",
    "\n",
    "        # Initialize weights randomly (earlier versions of thie code had this block inside MakeGraph, but that was a bug.)\n",
    "        maxWeight = -4.0 * np.sqrt(6.0 / (self.hiddenDimensions + self.visibleDimensions))\n",
    "        self.weights = tf.Variable(tf.random.uniform([self.visibleDimensions, self.hiddenDimensions], minval=-maxWeight, maxval=maxWeight), tf.float32, name=\"weights\")\n",
    "        self.hiddenBias = tf.Variable(tf.zeros([self.hiddenDimensions], tf.float32, name=\"hiddenBias\"))\n",
    "        self.visibleBias = tf.Variable(tf.zeros([self.visibleDimensions], tf.float32, name=\"visibleBias\"))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            trX = np.array(X)\n",
    "            for i in range(0, trX.shape[0], self.batchSize):\n",
    "                epochX = trX[i:i+self.batchSize]\n",
    "                self.MakeGraph(epochX)\n",
    "\n",
    "            print(\"Trained epoch \", epoch)\n",
    "\n",
    "    # get back rating predictions for a given user\n",
    "    def GetRecommendations(self, inputUser):\n",
    "        \n",
    "        feed = self.MakeHidden(inputUser)\n",
    "        rec = self.MakeVisible(feed)\n",
    "        return rec[0]       \n",
    "\n",
    "    def MakeGraph(self, inputUser):\n",
    "        \n",
    "        # Perform Gibbs Sampling for Contrastive Divergence, per the paper we assume k=1 instead of iterating over the \n",
    "        # forward pass multiple times since it seems to work just fine\n",
    "        \n",
    "        # Forward pass\n",
    "        # Sample hidden layer given visible...\n",
    "        # Get tensor of hidden probabilities\n",
    "        hProb0 = tf.nn.sigmoid(tf.matmul(inputUser, self.weights) + self.hiddenBias)\n",
    "        # Sample from all of the distributions\n",
    "        hSample = tf.nn.relu(tf.sign(hProb0 - tf.random.uniform(tf.shape(hProb0))))\n",
    "        # Stitch it together\n",
    "        forward = tf.matmul(tf.transpose(inputUser), hSample)\n",
    "        \n",
    "        # Backward pass\n",
    "        # Reconstruct visible layer given hidden layer sample\n",
    "        v = tf.matmul(hSample, tf.transpose(self.weights)) + self.visibleBias\n",
    "        \n",
    "        # Build up our mask for missing ratings\n",
    "        vMask = tf.sign(inputUser) # Make sure everything is 0 or 1\n",
    "        # User + movie + indicator of movie been rated\n",
    "        vMask3D = tf.reshape(vMask, [tf.shape(v)[0], -1, self.ratingValues]) # Reshape into arrays of individual ratings\n",
    "        vMask3D = tf.reduce_max(vMask3D, axis=[2], keepdims=True) # Use reduce_max to either give us 1 for ratings that exist, and 0 for missing ratings\n",
    "        \n",
    "        # Extract rating vectors for each individual set of 10 rating binary values\n",
    "        v = tf.reshape(v, [tf.shape(v)[0], -1, self.ratingValues])\n",
    "        vProb = tf.nn.softmax(v * vMask3D) # Apply softmax activation function\n",
    "        vProb = tf.reshape(vProb, [tf.shape(v)[0], -1]) # And shove them back into the flattened state. Reconstruction is done now.\n",
    "        # vProb = reconstructed data\n",
    "\n",
    "        # Stitch it together to define the backward pass and updated hidden biases\n",
    "        hProb1 = tf.nn.sigmoid(tf.matmul(vProb, self.weights) + self.hiddenBias)\n",
    "        backward = tf.matmul(tf.transpose(vProb), hProb1)\n",
    "    \n",
    "        # Now define what each epoch will do...\n",
    "        # Run the forward and backward passes, and update the weights\n",
    "        weightUpdate = self.weights.assign_add(self.learningRate * (forward - backward))\n",
    "        # Update hidden bias, minimizing the divergence in the hidden nodes\n",
    "        hiddenBiasUpdate = self.hiddenBias.assign_add(self.learningRate * tf.reduce_mean(hProb0 - hProb1, 0))\n",
    "        # Update the visible bias, minimizng divergence in the visible results\n",
    "        visibleBiasUpdate = self.visibleBias.assign_add(self.learningRate * tf.reduce_mean(inputUser - vProb, 0))\n",
    "\n",
    "        self.update = [weightUpdate, hiddenBiasUpdate, visibleBiasUpdate]\n",
    "        \n",
    "    def MakeHidden(self, inputUser):\n",
    "        hidden = tf.nn.sigmoid(tf.matmul(inputUser, self.weights) + self.hiddenBias)\n",
    "        self.MakeGraph(inputUser)\n",
    "        return hidden\n",
    "    \n",
    "    def MakeVisible(self, feed):\n",
    "        visible = tf.nn.sigmoid(tf.matmul(feed, tf.transpose(self.weights)) + self.visibleBias)\n",
    "        #self.MakeGraph(feed)\n",
    "        return visible\n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "        \n",
    "        rating = self.predictedRatings[u, i]\n",
    "        \n",
    "        if (rating < 0.001):\n",
    "            raise PredictionImpossible('No valid prediction exists.')\n",
    "            \n",
    "        return rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5260f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained epoch  0\n",
      "Trained epoch  1\n",
      "Trained epoch  2\n",
      "Trained epoch  3\n",
      "Trained epoch  4\n",
      "Trained epoch  5\n",
      "Trained epoch  6\n",
      "Trained epoch  7\n",
      "Trained epoch  8\n",
      "Trained epoch  9\n",
      "Trained epoch  10\n",
      "Trained epoch  11\n",
      "Trained epoch  12\n",
      "Trained epoch  13\n",
      "Trained epoch  14\n",
      "Trained epoch  15\n",
      "Trained epoch  16\n",
      "Trained epoch  17\n",
      "Trained epoch  18\n",
      "Trained epoch  19\n",
      "Processing user  0\n",
      "Processing user  50\n",
      "Processing user  100\n",
      "Processing user  150\n",
      "Processing user  200\n",
      "Processing user  250\n",
      "Processing user  300\n",
      "Processing user  350\n",
      "Processing user  400\n",
      "Processing user  450\n",
      "Processing user  500\n",
      "Processing user  550\n",
      "Processing user  600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RBM_v2 at 0x1df1b7b5d30>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RBM with (num items * rating values) visible nodes\n",
    "model_v2 = RBM_v2(trainingMatrix.shape[1])\n",
    "\n",
    "# Fit the RBM model on the training set\n",
    "model_v2.fit(trainSet, trainingMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e32d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predtest = model_v2.test(testSet)\n",
    "predantitest = model_v2.test(antitest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bd311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551af26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e874630",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a675b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../metrics')\n",
    "from metrics import evaluationMetrics\n",
    "em = evaluationMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ad077ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test and antitest predictions\n",
    "# predtest = model.test(testSet)\n",
    "# predantitest = model.test(antitest)\n",
    "\n",
    "# Get top N recommended movies for each user based on estimated ratings\n",
    "top_10_RBM = em.getTopN(predantitest, minimumRating = 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490e401",
   "metadata": {},
   "source": [
    "## Métricas de precisión: RMSE y MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dab496c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1618\n",
      "MAE:  0.9639\n"
     ]
    }
   ],
   "source": [
    "# # RMSE\n",
    "rmse = accuracy.rmse(predtest)\n",
    "\n",
    "# MAE\n",
    "mae = accuracy.mae(predtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6284258",
   "metadata": {},
   "source": [
    "## Métricas de relevancia: Precision, Recall y NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "745ece3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "precisions = em.getPrecision(predtest, k=10, threshold=3.5)\n",
    "\n",
    "# Mean Average Precision\n",
    "mapModel = np.mean(list(precisions.values()))\n",
    "\n",
    "# Recall\n",
    "recalls = em.getRecall(predtest, k=10, threshold=3.5)\n",
    "\n",
    "# Mean Average Recall\n",
    "marModel = np.mean(list(recalls.values()))\n",
    "\n",
    "# Normalized discounted cummulative gain (NDCG)\n",
    "ndcgs, mean_ndcg = em.getNDCG(predtest,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5570aa",
   "metadata": {},
   "source": [
    "## Otras métricas de interés: Coverage, User Coverage y Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72d16ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m user_coverage \u001b[38;5;241m=\u001b[39m em\u001b[38;5;241m.\u001b[39mgetUserCoverage(top_10_RBM, trainSet\u001b[38;5;241m.\u001b[39mn_users,\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Novelty\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m novelty \u001b[38;5;241m=\u001b[39m \u001b[43mem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNovelty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_10_RBM\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainSet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Github - JAVA\\RecommenderSys\\metrics\\metrics.py:270\u001b[0m, in \u001b[0;36mevaluationMetrics.getNovelty\u001b[1;34m(self, topNPredicted, trainset)\u001b[0m\n\u001b[0;32m    267\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rank\n\u001b[0;32m    268\u001b[0m         n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 270\u001b[0m novelty \u001b[38;5;241m=\u001b[39m \u001b[43mtotal\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mn\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m novelty\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Coverage\n",
    "coverage = em.getCoverage(top_10_RBM,trainSet.n_items,trainSet.all_users())\n",
    "\n",
    "# User coverage\n",
    "user_coverage = em.getUserCoverage(top_10_RBM, trainSet.n_users,4)\n",
    "\n",
    "# Novelty\n",
    "novelty = em.getNovelty(top_10_RBM,trainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505550d3",
   "metadata": {},
   "source": [
    "Por último creamos un dataframe con todas las métricas de evaluación asociadas al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00bf509e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAR</th>\n",
       "      <th>Mean_NDCG</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>User_Coverage</th>\n",
       "      <th>Novelty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RBM_v2</td>\n",
       "      <td>1.161793</td>\n",
       "      <td>0.963934</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.007933</td>\n",
       "      <td>0.936041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model      RMSE       MAE       MAP       MAR  Mean_NDCG  Coverage  \\\n",
       "0  RBM_v2  1.161793  0.963934  0.163041  0.007933   0.936041       0.0   \n",
       "\n",
       "   User_Coverage Novelty  \n",
       "0            0.0      NA  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"Model\",\"RMSE\",\"MAE\",\"MAP\",\"MAR\",\"Mean_NDCG\",\"Coverage\",\"User_Coverage\",\"Novelty\"]\n",
    "metrics_data = []\n",
    "\n",
    "# Append the results to the list of dictionaries\n",
    "metrics_data.append({\"Model\": \"RBM_v2\", \"RMSE\": rmse, \"MAE\": mae, \"MAP\": mapModel, \"MAR\": marModel,\n",
    "                     \"Mean_NDCG\": mean_ndcg, \"Coverage\": coverage, \"User_Coverage\": user_coverage,\n",
    "                     \"Novelty\": novelty})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data, columns=cols)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5e968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e49e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccab3394",
   "metadata": {},
   "source": [
    "# Random Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b631b",
   "metadata": {},
   "source": [
    "Vamos a evaluar también un modelo Random, en concreto \"NormalPredictor\" para poder comparar sus resultados con el resto de modelos. NormalPredictor es un algoritmo simple en Surprise que predice calificaciones aleatoriamente basado en la distribución del conjunto de entrenamiento. Supone una distribución normal de las calificaciones y genera predicciones aleatorias en función de esa distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e07f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "Random = NormalPredictor()\n",
    "Random.fit(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1850e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get test and antitest predictions\n",
    "predtest_random = Random.test(testSet)\n",
    "predantitest_random = Random.test(antitest)\n",
    "\n",
    "# Get top N recommended movies for each user based on estimated ratings\n",
    "top_10_random = em.getTopN(predantitest_random,minimumRating = 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695bf59f",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Metrics\n",
    "rmse_random = accuracy.rmse(predtest_random)\n",
    "mae_random = accuracy.mae(predtest_random)\n",
    "\n",
    "# Relevance metrics\n",
    "precisions_random = em.getPrecision(predtest_random, k=10, threshold=3.5)\n",
    "mapModel_random = np.mean(list(precisions_random.values()))\n",
    "\n",
    "recalls_random = em.getRecall(predtest_random, k=10, threshold=3.5)\n",
    "marModel_random = np.mean(list(recalls_random.values()))\n",
    "\n",
    "ndcgs_random, mean_ndcg_random = em.getNDCG(predtest_random,10)\n",
    "\n",
    "# Other metrics\n",
    "coverage_random = em.getCoverage(top_10_random,trainSet.n_items,trainSet.all_users())\n",
    "user_coverage_random = em.getUserCoverage(top_10_random, trainSet.n_users,4)\n",
    "novelty_random = em.getNovelty(top_10_random,trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfaa5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Model\",\"RMSE\",\"MAE\",\"MAP\",\"MAR\",\"Mean_NDCG\",\"Coverage\",\"User_Coverage\",\"Novelty\"]\n",
    "metrics_data = []\n",
    "\n",
    "# Append the results to the list of dictionaries\n",
    "metrics_data.append({\"Model\": \"random\", \"RMSE\": rmse_random, \"MAE\": mae_random, \"MAP\": mapModel_random, \"MAR\": marModel_random,\n",
    "                     \"Mean_NDCG\": mean_ndcg_random, \"Coverage\": coverage_random, \"User_Coverage\": user_coverage_random,\n",
    "                     \"Novelty\": novelty_random})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data, columns=cols)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to the dataframe with the metrics of all models.\n",
    "em.addToMetricsDataframe(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
